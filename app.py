import streamlit as st
import os
import time
import tempfile
import hashlib
import uuid

from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_pinecone import PineconeVectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from pinecone import Pinecone, ServerlessSpec

# --- 1. CONFIGURA√á√ÉO DA P√ÅGINA ---
st.set_page_config(page_title="IA Auditoria Municipal - Consulta Avan√ßada", layout="wide", page_icon="üèõÔ∏è")

st.markdown("""
    <style>
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    .stAlert {margin-top: 10px;}
    .source-box {
        background-color: #f0f2f6;
        border-radius: 5px;
        padding: 10px;
        margin: 5px 0;
        border-left: 4px solid #4CAF50;
    }
    </style>
    """, unsafe_allow_html=True)

# --- 2. CARREGAMENTO DE SEGREDOS COM VALIDA√á√ÉO ---
if "GOOGLE_API_KEY" not in st.secrets or "PINECONE_API_KEY" not in st.secrets:
    st.error("‚ùå ERRO: Chaves de API n√£o configuradas no secrets.toml!")
    st.stop()

os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
os.environ["PINECONE_API_KEY"] = st.secrets["PINECONE_API_KEY"]

# Configura√ß√µes do Pinecone
PINECONE_API_KEY = st.secrets["PINECONE_API_KEY"]
INDEX_NAME = "tcc-auditoria"

# --- 3. INICIALIZA√á√ÉO CORRETA DO PINECONE ---
@st.cache_resource
def init_pinecone():
    """Inicializa o cliente Pinecone corretamente"""
    try:
        pc = Pinecone(api_key=PINECONE_API_KEY)
        existing_indexes = [index.name for index in pc.list_indexes()]
        
        if INDEX_NAME not in existing_indexes:
            st.info(f"üîÑ √çndice '{INDEX_NAME}' n√£o encontrado. Criando...")
            pc.create_index(
                name=INDEX_NAME,
                dimension=768,  # Dimens√£o do embedding Gemini
                metric="cosine",
                spec=ServerlessSpec(
                    cloud="aws",
                    region="us-east-1"  # Ajuste conforme seu ambiente Pinecone
                )
            )
            time.sleep(10)
            st.success(f"‚úÖ √çndice '{INDEX_NAME}' criado com sucesso!")
        
        return pc
    except Exception as e:
        st.error(f"‚ùå Erro ao inicializar Pinecone: {str(e)}")
        return None

@st.cache_resource
def get_vectorstore():
    """Conecta ao Pinecone com configura√ß√µes otimizadas"""
    try:
        # Nota: models/embedding-001 √© o nome padr√£o correto para embeddings textuais do Gemini
        embeddings = GoogleGenerativeAIEmbeddings(
            model="models/gemini-embedding-001",
            task_type="retrieval_query"  # Otimizado para consulta
        )
        
             
        
        vectorstore = PineconeVectorStore(
            index_name=INDEX_NAME,
            embedding=embeddings,
            pinecone_api_key=PINECONE_API_KEY
        )
        return vectorstore
    except Exception as e:
        st.error(f"‚ùå Erro ao conectar ao vectorstore: {str(e)}")
        return None

@st.cache_resource
def get_llm():
    """Inicializa o modelo de linguagem Gemini"""
    return ChatGoogleGenerativeAI(
        model="gemini-2.0-flash", 
        temperature=0.1,
        max_retries=1  # <-- ADICIONE ISTO AQUI
    )


# --- 4. PROCESSAMENTO DE PDF ---
def process_pdf_otimizado(uploaded_file):
    """Processamento otimizado de PDFs"""
    tmp_file_path = None
    try:
        if uploaded_file is None:
            return False, "‚ùå Nenhum arquivo fornecido."
        
        file_content = uploaded_file.read()
        if len(file_content) == 0:
            return False, "‚ùå Arquivo vazio."
        
        file_hash = hashlib.md5(file_content).hexdigest()
        uploaded_file.seek(0)
        
        vectorstore = get_vectorstore()
        if vectorstore is None:
            return False, "‚ùå N√£o foi poss√≠vel conectar ao banco de dados."
        
        try:
            existing = vectorstore.similarity_search(
                "dummy query",
                k=1,
                filter={"file_hash": {"$eq": file_hash}}
            )
            if existing:
                return False, "‚ö†Ô∏è Documento j√° processado anteriormente."
        except Exception as e:
            st.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel verificar duplicidade: {str(e)}")
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(file_content)
            tmp_file_path = tmp_file.name
        
        try:
            loader = PyPDFLoader(tmp_file_path)
            docs = loader.load()
        except Exception as e:
            return False, f"‚ùå Erro ao ler PDF: {str(e)}"
        
        if not docs:
            return False, "‚ùå PDF vazio ou sem texto extra√≠vel."
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        splits = text_splitter.split_documents(docs)
        
        documentos_para_adicionar = []
        for i, split in enumerate(splits):
            chunk_id = str(uuid.uuid4())
            split.metadata.update({
                "file_hash": file_hash,
                "source": uploaded_file.name,
                "chunk_index": i,
                "total_chunks": len(splits),
                "doc_type": "PDF",
                "id": chunk_id
            })
            if split.page_content and len(split.page_content.strip()) > 0:
                documentos_para_adicionar.append(split)
        
        if not documentos_para_adicionar:
            return False, "‚ùå Nenhum conte√∫do v√°lido extra√≠do do PDF."
        
        total = len(documentos_para_adicionar)
        progress_bar = st.progress(0, text="Enviando para o Pinecone...")
        
        batch_size = 10
        for i in range(0, total, batch_size):
            batch = documentos_para_adicionar[i:i + batch_size]
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    vectorstore.add_documents(batch)
                    break
                except Exception as e:
                    if attempt < max_retries - 1:
                        time.sleep(2 ** attempt)
                    else:
                        raise e
            progress = min((i + batch_size) / total, 1.0)
            progress_bar.progress(progress, text=f"Enviando... {int(progress * 100)}%")
        
        progress_bar.empty()
        
        if tmp_file_path and os.path.exists(tmp_file_path):
            os.remove(tmp_file_path)
            
        return True, f"‚úÖ Sucesso! {total} partes indexadas no Pinecone."
        
    except Exception as e:
        if tmp_file_path and os.path.exists(tmp_file_path):
            try:
                os.remove(tmp_file_path)
            except:
                pass
        return False, f"‚ùå Erro durante o processamento: {str(e)}"

# --- 5. INTERFACE DO USU√ÅRIO ---
def main():
    pc = init_pinecone()
    if pc is None:
        st.error("‚ùå N√£o foi poss√≠vel inicializar o Pinecone. Verifique suas credenciais.")
        return
    
    query_params = st.query_params
    modo = query_params.get("mode", "cidadao")
    
    # Sidebar
    with st.sidebar:
        st.title("üèõÔ∏è Painel de Controle")
        
        if modo == "admin":
            st.success("üîí MODO ADMINISTRADOR")
            st.markdown("---")
            st.subheader("üì§ Upload de Documentos")
            
            uploaded_file = st.file_uploader("Selecione o PDF", type="pdf")
            if uploaded_file and st.button("üöÄ Processar Documento", use_container_width=True):
                with st.spinner("Processando documento..."):
                    sucesso, msg = process_pdf_otimizado(uploaded_file)
                    if sucesso:
                        st.success(msg)
                        st.balloons()
                    else:
                        st.error(msg)
            
            st.markdown("---")
            st.subheader("üìä Estat√≠sticas")
            st.metric("Status", "Conectado" if pc else "Desconectado")
            st.metric("√çndice", INDEX_NAME)
    
    # √Årea principal
    st.title("ü§ñ Assistente Virtual da Prefeitura")
    st.caption("Consultas baseadas exclusivamente em documentos oficiais (RAG + Pinecone)")
    
    # Hist√≥rico de Chat
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Input do usu√°rio
    if prompt := st.chat_input("Digite sua d√∫vida sobre os documentos municipais..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        with st.chat_message("assistant"):
            with st.spinner("üîç Consultando base documental e gerando resposta..."):
                try:
                    vectorstore = get_vectorstore()
                    llm = get_llm()
                    
                    if vectorstore and llm:
                        # 1. Configurar o Retriever do Pinecone
                        retriever = vectorstore.as_retriever(
                            search_type="similarity",
                            search_kwargs={"k": 5, "filter": {"doc_type": "PDF"}}
                        )
                        
                        # 2. Criar o Prompt que OBRIGA a IA a usar o Pinecone
                        system_prompt = (
                            "Voc√™ √© um assistente virtual da Auditoria Municipal. "
                            "Sua fun√ß√£o √© responder √†s perguntas baseando-se EXCLUSIVAMENTE nos documentos de contexto fornecidos abaixo. "
                            "Se a resposta n√£o estiver nos documentos fornecidos, responda exatamente: "
                            "'Desculpe, n√£o encontrei informa√ß√µes sobre isso nos documentos oficiais anexados.' "
                            "N√£o invente informa√ß√µes ou use seu conhecimento pr√©vio.\n\n"
                            "Contexto recuperado dos documentos:\n{context}"
                        )
                        
                        prompt_template = ChatPromptTemplate.from_messages([
                            ("system", system_prompt),
                            ("human", "{input}")
                        ])
                        
                        # 3. Montar a Cadeia RAG
                        question_answer_chain = create_stuff_documents_chain(llm, prompt_template)
                        rag_chain = create_retrieval_chain(retriever, question_answer_chain)
                        
                        # 4. Executar a consulta
                        response = rag_chain.invoke({"input": prompt})
                        
                        answer = response["answer"]
                        source_docs = response["context"]
                        
                        # Exibe a resposta formulada pela IA
                        st.markdown(answer)
                        
                        # Exibe as fontes de onde ela tirou a informa√ß√£o
                        if source_docs:
                            st.markdown("---")
                            st.markdown("üìö **Trechos Consultados:**")
                            for i, doc in enumerate(source_docs):
                                fonte = doc.metadata.get('source', 'Fonte desconhecida')
                                with st.expander(f"üìÑ Fonte {i+1} - {fonte}"):
                                    st.markdown(f"*{doc.page_content}*")
                        else:
                            st.warning("‚ö†Ô∏è Nenhum documento PDF relevante foi encontrado no banco de dados para esta consulta.")
                        
                        # Salva no hist√≥rico
                        st.session_state.messages.append({"role": "assistant", "content": answer})
                    else:
                        st.error("Erro interno: Falha ao carregar banco de dados vetorial ou modelo LLM.")
                        
                except Exception as e:
                    st.error(f"Erro durante a gera√ß√£o da resposta: {str(e)}")

if __name__ == "__main__":
    main()

